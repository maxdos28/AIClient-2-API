# AI Proxy - Rust Edition ðŸ¦€

A high-performance AI API proxy written in Rust, supporting multiple AI providers with automatic protocol conversion.

## Features

- âœ… **Multi-Provider Support**: OpenAI, Claude, Gemini
- âœ… **Automatic Protocol Conversion**: Seamlessly convert between different API formats
- âœ… **High Performance**: Built with Rust for maximum speed and efficiency  
- âœ… **Async/Await**: Fully asynchronous using Tokio
- âœ… **Type Safety**: Leverages Rust's type system for reliability
- âœ… **Memory Safe**: No segfaults, no data races
- âœ… **In-Memory Caching**: Fast response caching with TTL support
- âœ… **Low Resource Usage**: Minimal memory footprint

## Performance

```
Binary Size: 6.8 MB (optimized release build)
Memory Usage: < 10 MB at idle
Request Latency: Sub-millisecond protocol conversion
```

## Quick Start

### Build

```bash
cargo build --release
```

### Run

```bash
# With OpenAI
./target/release/aiproxy --openai-api-key sk-xxx

# With Claude
./target/release/aiproxy --claude-api-key claude-xxx

# With multiple providers
./target/release/aiproxy \
  --openai-api-key sk-xxx \
  --claude-api-key claude-xxx \
  --gemini-api-key gemini-xxx

# Custom host and port
./target/release/aiproxy \
  --host 127.0.0.1 \
  --port 8080 \
  --openai-api-key sk-xxx
```

### Using Environment Variables

```bash
export OPENAI_API_KEY=sk-xxx
export CLAUDE_API_KEY=claude-xxx
export GEMINI_API_KEY=gemini-xxx

./target/release/aiproxy
```

## API Usage

### Health Check

```bash
curl http://localhost:3000/health
```

### Chat Completion

```bash
curl -X POST http://localhost:3000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-3.5-turbo",
    "messages": [
      {"role": "user", "content": "Hello!"}
    ]
  }'
```

### List Models

```bash
curl http://localhost:3000/v1/models
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Axum Web Server   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Protocol Converter  â”‚
â”‚  (OpenAI â†” Claude)   â”‚
â”‚  (Claude â†” Gemini)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Provider Client    â”‚
â”‚  (OpenAI/Claude/     â”‚
â”‚   Gemini)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Testing

```bash
# Run all tests
cargo test

# Run tests with output
cargo test -- --nocapture

# Run specific test
cargo test test_openai_to_claude

# Run benchmarks
cargo bench
```

## Code Structure

```
rust-aiproxy/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.rs           # Entry point
â”‚   â”œâ”€â”€ lib.rs            # Library root
â”‚   â”œâ”€â”€ error.rs          # Error handling
â”‚   â”œâ”€â”€ models.rs         # Data models (600+ lines)
â”‚   â”œâ”€â”€ converter.rs      # Protocol conversion (400+ lines)
â”‚   â”œâ”€â”€ providers.rs      # Provider implementations
â”‚   â”œâ”€â”€ server.rs         # HTTP server
â”‚   â””â”€â”€ cache.rs          # Caching layer
â”œâ”€â”€ Cargo.toml            # Dependencies
â””â”€â”€ README.md
```

## Dependencies

- **axum**: Web framework
- **tokio**: Async runtime
- **serde**: Serialization
- **reqwest**: HTTP client (with rustls)
- **tower**: Middleware
- **clap**: CLI argument parsing
- **uuid**: UUID generation
- **chrono**: Date/time handling
- **anyhow**: Error handling

## Development

```bash
# Format code
cargo fmt

# Lint code
cargo clippy

# Check compilation
cargo check

# Watch for changes
cargo watch -x run
```

## Comparison with Other Versions

| Feature | Rust | Go | Node.js |
|---------|------|----|---------| 
| Binary Size | 6.8 MB | 22 MB | N/A |
| Memory Usage | ~10 MB | ~50 MB | ~100 MB |
| Cold Start | < 1ms | ~10ms | ~50ms |
| Type Safety | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜†â˜†â˜† |
| Performance | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜†â˜† |
| Development Speed | â˜…â˜…â˜…â˜†â˜† | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜…â˜… |

## License

MIT

## Contributing

Contributions welcome! Please open an issue or PR.

## Roadmap

- [ ] WebSocket support for streaming
- [ ] Redis-based distributed caching
- [ ] Prometheus metrics endpoint
- [ ] Load balancing across multiple instances
- [ ] Rate limiting
- [ ] Request/response logging
- [ ] Token counting and billing

## Author

AI Proxy Team
